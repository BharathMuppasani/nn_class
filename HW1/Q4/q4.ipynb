{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rules in Artificial Neural Networks (ANN)\n",
    "\n",
    "Artificial Neural Networks (ANNs) use various learning rules to update weights and biases, enhancing network performance. Below are some common learning rules used in ANNs:\n",
    "\n",
    "### 1. Hebbian Learning Rule\n",
    "Hebbian learning is an unsupervised learning rule where the connection (weight) between neurons is strengthened when they activate together.\n",
    "- **Weight Update Rule**:  \n",
    "  $$ \\delta w = \\alpha x_i y $$  \n",
    "  Where:  \n",
    "  - $$ \\delta w = \\text{change in weight} $$  \n",
    "  - $$ \\alpha = \\text{learning rate} $$  \n",
    "  - $$ x_i = \\text{input vector} $$  \n",
    "  - $$ y = \\text{output} $$\n",
    "\n",
    "\n",
    "### 2. Perceptron Learning Rule\n",
    "The Perceptron learning rule is supervised and adjusts weights based on the difference between actual and desired output. If an error exists, the weights are corrected. This is mainly used for binary classification.\n",
    "- **Weight Update Rule**:  \n",
    "  $$ \\delta w = \\alpha x_i e_j $$  \n",
    "  Where:  \n",
    "  - $$ e_j = t_i - y  $$ \n",
    "    (difference between desired and actual output) \n",
    "  - $$ x_i = \\text{input vector} $$  \n",
    "  - $$ \\alpha = \\text{learning rate} $$  \n",
    "- **Output Decision**:  \n",
    "  $$ \n",
    "  y = \n",
    "  \\begin{cases} \n",
    "  1, & \\text{if net input} \\geq \\theta \\\\\n",
    "  0, & \\text{if net input} < \\theta \n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "### 3. Delta Learning Rule (LMS Rule)\n",
    "The Delta rule, also known as the Least Mean Squares (LMS) rule, is a supervised learning method that uses gradient descent to minimize the error between the desired and actual output. It is typically used for continuous activation functions.\n",
    "- **Weight Update Rule**:  \n",
    "  $$ \\delta w = \\alpha x_i (t_i - y) y' $$  \n",
    "  Where:  \n",
    "  - $$ t_i = \\text{target output} $$  \n",
    "  - $$ y = \\text{actual output} $$  \n",
    "  - $$ y' = \\text{derivative of the output} $$  \n",
    "  - $$ \\alpha = \\text{learning rate} $$  \n",
    "  - $$ x_i = \\text{input vector} $$\n",
    "\n",
    "### 4. Correlation Learning Rule\n",
    "The correlation learning rule is similar to Hebbian learning but is supervised. The weight between two neurons is updated based on both the input and target output. When neurons activate together, the connection strengthens.\n",
    "- **Weight Update Rule**:  \n",
    "  $$ \\delta w = \\alpha x_i t_j $$  \n",
    "  Where:  \n",
    "  - $$ t_j = \\text{target output} $$  \n",
    "  - $$ x_i = \\text{input vector} $$  \n",
    "  - $$ \\alpha = \\text{learning rate} $$\n",
    "\n",
    "### 5. Out Star Learning Rule\n",
    "This rule is used in networks with layers and is supervised. The weights connected to a specific neuron are adjusted to match the target outputs for the neurons in the next layer.\n",
    "- **Weight Update Rule**:  \n",
    "  $$ \\delta w = \\alpha (t - y) $$  \n",
    "  Where:  \n",
    "  - $$ t = \\text{target output} $$  \n",
    "  - $$ y = \\text{actual output} $$  \n",
    "  - $$ \\alpha = \\text{learning rate} $$\n",
    "\n",
    "### 6. Competitive Learning Rule\n",
    "Also known as the \"Winner-takes-All\" rule, this unsupervised learning method updates the weights of only the most activated (winning) neuron. The winning neuron competes to represent the input pattern, while the weights of other neurons remain unchanged.\n",
    "- **Weight Update Rule**: Only the neuron with the strongest activation updates its weights.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
