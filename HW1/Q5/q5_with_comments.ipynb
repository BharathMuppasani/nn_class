{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c8db73",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries\n",
    "In this cell, we are importing all the required libraries, such as:\n",
    "- `numpy` for numerical computations\n",
    "- `torch` and `torch_geometric` for deep learning and graph neural networks\n",
    "- `matplotlib` and `seaborn` for data visualization\n",
    "- `tqdm` for progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/727cj6hn4y51l1y4yhdfc6880000gn/T/ipykernel_63473/3482116912.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Layer Explanation\n",
    "\n",
    "This code defines a Graph Convolutional Network (GCN) layer that updates node features by aggregating information from neighboring nodes in a graph.\n",
    "\n",
    "- **Initialization (`__init__`)**:  \n",
    "  A linear layer (`nn.Linear`) is defined to transform the input node features from `c_in` dimensions to `c_out` dimensions.\n",
    "\n",
    "- **Forward Method**:  \n",
    "  The forward pass takes `node_feats` (node features) and `adj_matrix` (graph adjacency matrix) as inputs.\n",
    "  - **Neighbor Calculation**:  \n",
    "    The sum of edges for each node is computed (`adj_matrix.sum(dim=-1)`), which gives the number of neighbors.\n",
    "  - **Projection**:  \n",
    "    Node features are transformed using a linear projection.\n",
    "  - **Aggregation**:  \n",
    "    Node features are aggregated from neighbors using matrix multiplication (`torch.bmm`).\n",
    "  - **Normalization**:  \n",
    "    The aggregated features are normalized by the number of neighbors.\n",
    "\n",
    "- **Output**:  \n",
    "  Returns the updated node features with shape `[batch_size, num_nodes, c_out]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Features and Adjacency Matrix Setup\n",
    "\n",
    "This code creates:\n",
    "1. **Node Features (`node_feats`)**: A tensor representing 4 nodes, each with 2 features, in a single batch. The tensor is created using `torch.arange(8)` and reshaped to `[1, 4, 2]`.\n",
    "   \n",
    "2. **Adjacency Matrix (`adj_matrix`)**: A 4x4 adjacency matrix representing the connections between nodes in a graph. A value of `1` indicates an edge between nodes, and `0` indicates no edge. The adjacency matrix is reshaped to include a batch dimension, with shape `[1, 4, 4]`.\n",
    "\n",
    "This setup is used to represent a small graph with 4 nodes and the connectivity between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Layer Forward Pass\n",
    "\n",
    "In this section, a GCN layer is initialized and applied to the `node_feats` and `adj_matrix`. \n",
    "\n",
    "1. **Layer Initialization**:  \n",
    "   A `GCNLayer` is created with 2 input features (`c_in=2`) and 2 output features (`c_out=2`). \n",
    "   The layer's weight and bias for the linear projection are manually set:\n",
    "   - Weight matrix is set to the identity matrix `[[1, 0], [0, 1]]`.\n",
    "   - Bias vector is set to `[0, 0]`, meaning no bias is applied.\n",
    "\n",
    "2. **Forward Pass**:  \n",
    "   Using `torch.no_grad()`, the GCN layer processes the input features (`node_feats`) and adjacency matrix (`adj_matrix`). The output node features are calculated based on the graph structure and the feature transformation defined by the layer.\n",
    "\n",
    "3. **Output**:  \n",
    "   The adjacency matrix, input features, and output features are printed for inspection. The output features represent the updated node representations after the GCN layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT Layer Implementation\n",
    "\n",
    "This code defines a **Graph Attention Network (GAT) layer**. GAT is an extension of GCN that introduces attention mechanisms to weight the importance of neighboring nodes for feature aggregation.\n",
    "\n",
    "#### Key Elements:\n",
    "1. **Initialization (`__init__`)**:\n",
    "    - **Input Parameters**:\n",
    "      - `c_in`: Dimensionality of input node features.\n",
    "      - `c_out`: Dimensionality of output node features.\n",
    "      - `num_heads`: Number of attention heads used in parallel.\n",
    "      - `concat_heads`: Whether to concatenate or average the output of the attention heads.\n",
    "      - `alpha`: Negative slope for the LeakyReLU activation function used in the attention mechanism.\n",
    "    - **Projection Layer**: A linear projection is applied to node features to transform them into output space (`nn.Linear(c_in, c_out * num_heads)`).\n",
    "    - **Attention Mechanism (`self.a`)**: Learnable parameters for computing attention scores between nodes, applied independently for each head.\n",
    "    - **Initialization**: Weights and attention parameters are initialized using Xavier initialization (`nn.init.xavier_uniform_`).\n",
    "\n",
    "2. **Forward Method**:\n",
    "    The forward pass involves the following steps:\n",
    "    - **Linear Projection**: Node features are transformed using the linear projection, and the output is divided between the attention heads.\n",
    "    - **Attention Calculation**:\n",
    "      - The adjacency matrix is used to retrieve the edges (non-zero values). For each edge, attention scores are computed between the source and target nodes.\n",
    "      - Attention logits are calculated using the dot product between the transformed node features, followed by a LeakyReLU activation.\n",
    "    - **Attention Weighting**: The attention logits are used to create an attention matrix, and a softmax is applied to normalize the attention scores. These scores determine the weight of each neighboring node during aggregation.\n",
    "    - **Feature Aggregation**: Node features are aggregated using the attention scores (`torch.einsum`). Each node aggregates its neighbors' features weighted by the attention scores.\n",
    "    - **Concatenation or Averaging**: If `concat_heads=True`, the features from all heads are concatenated. Otherwise, the features are averaged across heads.\n",
    "\n",
    "3. **Return**:\n",
    "    The layer returns the updated node features, which have been aggregated and transformed based on the attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT Layer Forward Pass with Multiple Attention Heads\n",
    "\n",
    "1. **Layer Initialization**:  \n",
    "   A `GATLayer` is initialized with 2 input features, 2 output features, and 2 attention heads (`num_heads=2`). The layer's projection weights and biases are manually set:\n",
    "   - Weight matrix is set to the identity matrix `[[1, 0], [0, 1]]`, so no feature transformation is applied.\n",
    "   - Bias is set to `[0, 0]`, meaning no additional bias is applied.\n",
    "   - The attention mechanism parameters (`self.a`) are manually set to specific values for both heads.\n",
    "\n",
    "2. **Forward Pass**:  \n",
    "   The forward pass is performed without gradients (`torch.no_grad()`), and the attention probabilities are printed (`print_attn_probs=True`). This operation calculates the attention scores between nodes and aggregates features accordingly:\n",
    "   - The input node features are projected and transformed.\n",
    "   - Attention scores are computed for the edges in the adjacency matrix.\n",
    "   - Features from neighboring nodes are aggregated using the computed attention scores.\n",
    "\n",
    "3. **Output**:\n",
    "   - **Adjacency Matrix**: The connectivity of the graph is displayed, showing which nodes are connected.\n",
    "   - **Input Features**: The node features before applying the GAT layer.\n",
    "   - **Output Features**: The updated node features after applying the attention-based aggregation from neighboring nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8f14c",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Layer Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc701cd",
   "metadata": {},
   "source": [
    "### Loading the Cora Dataset\n",
    "Here, the Cora dataset is loaded using the `torch_geometric` library. The Cora dataset is often used in graph-based deep learning tasks. The dataset is a citation network where nodes represent documents and edges represent citations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNNModel: General Graph Neural Network Model\n",
    "\n",
    "This class defines a flexible Graph Neural Network (GNN) model that can utilize different GNN layers like GCN, GAT, or GraphConv. The architecture allows easy customization of the number of layers, layer types, and other hyperparameters.\n",
    "\n",
    "#### Key Elements:\n",
    "1. **Initialization (`__init__`)**:\n",
    "    - **Input Parameters**:\n",
    "      - `c_in`: Dimensionality of the input node features.\n",
    "      - `c_hidden`: Dimensionality of the hidden features (intermediate layers).\n",
    "      - `c_out`: Dimensionality of the output features (e.g., the number of classes for classification tasks).\n",
    "      - `num_layers`: The number of layers in the GNN model.\n",
    "      - `layer_name`: Specifies which GNN layer to use (e.g., \"GCN\", \"GAT\").\n",
    "      - `dp_rate`: Dropout rate for regularization.\n",
    "      - `kwargs`: Additional arguments specific to certain layers (e.g., the number of attention heads for GAT).\n",
    "    - **Layer Construction**: \n",
    "      - The GNN layers are built dynamically based on the `layer_name` provided. The layers alternate between GNN layers, ReLU activations, and dropout for regularization.\n",
    "      - The final layer is a GNN layer that outputs `c_out` features.\n",
    "\n",
    "2. **Forward Pass (`forward`)**:\n",
    "    - **Inputs**:\n",
    "      - `x`: Input features for each node.\n",
    "      - `edge_index`: The edge list in PyTorch Geometric format, which specifies the graph's connectivity.\n",
    "    - **Layer Application**:\n",
    "      - The forward pass iterates over the defined layers. For each layer:\n",
    "        - If the layer is a GNN layer (inheriting from `MessagePassing`), the `edge_index` is provided as an additional argument.\n",
    "        - Otherwise, it's treated as a regular layer (e.g., ReLU or Dropout) and applied to the node features.\n",
    "    - **Output**: The final output contains the updated node features after all layers have been applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPModel: Multi-Layer Perceptron Model\n",
    "\n",
    "This class defines a basic **Multi-Layer Perceptron (MLP)** model, which is a fully connected feed-forward neural network. It consists of multiple hidden layers and supports regularization through dropout.\n",
    "\n",
    "#### Key Elements:\n",
    "1. **Initialization (`__init__`)**:\n",
    "    - **Input Parameters**:\n",
    "      - `c_in`: Dimensionality of the input features.\n",
    "      - `c_hidden`: Dimensionality of the hidden features.\n",
    "      - `c_out`: Dimensionality of the output features (usually representing the number of classes in classification tasks).\n",
    "      - `num_layers`: Number of hidden layers in the MLP.\n",
    "      - `dp_rate`: Dropout rate applied after each hidden layer for regularization.\n",
    "    - **Layer Construction**:\n",
    "      - The model is built by stacking linear layers, ReLU activations, and dropout layers for hidden layers.\n",
    "      - The final layer is a linear layer that outputs `c_out` features, without any activation or dropout.\n",
    "      - All the layers are combined into a single sequential module (`nn.Sequential`).\n",
    "\n",
    "2. **Forward Pass (`forward`)**:\n",
    "    - **Inputs**:\n",
    "      - `x`: Input features for the MLP, typically node features in a graph.\n",
    "    - **Output**: The input `x` is passed through the sequentially stacked layers, and the final result is returned after the last layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of hidden layers\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687f6a3",
   "metadata": {},
   "source": [
    "### NodeLevelGNN: Node-Level Graph Neural Network using PyTorch Lightning\n",
    "\n",
    "This class implements a **Node-Level Graph Neural Network (GNN)** for tasks like node classification using PyTorch Lightning. It allows for flexible model selection between GNN-based architectures and MLPs, and handles the training, validation, and testing of the model on graph-structured data.\n",
    "\n",
    "#### Key Elements:\n",
    "1. **Initialization (`__init__`)**:\n",
    "    - **Input Parameters**:\n",
    "      - `model_name`: The name of the model to be used, either `\"MLP\"` or a GNN model like `\"GCN\"`, `\"GAT\"`, etc.\n",
    "      - `model_kwargs`: Additional keyword arguments for the model, such as the number of layers, hidden dimensions, and other hyperparameters.\n",
    "    - **Model Selection**:\n",
    "      - If `model_name` is `\"MLP\"`, the Multi-Layer Perceptron (`MLPModel`) is used.\n",
    "      - Otherwise, the specified GNN model (`GNNModel`) is initialized.\n",
    "    - A cross-entropy loss module (`nn.CrossEntropyLoss`) is set up to compute classification loss during training and evaluation.\n",
    "\n",
    "2. **Forward Pass (`forward`)**:\n",
    "    - **Inputs**:\n",
    "      - `data`: The graph data, containing node features (`x`), edge index (`edge_index`), and masks for training, validation, and test splits.\n",
    "      - `mode`: Specifies the mode of operation (`train`, `val`, or `test`) to compute the loss and accuracy only on the relevant nodes based on the provided masks.\n",
    "    - **Model Execution**: The input features and graph structure are passed through the selected model.\n",
    "    - **Masking and Loss Calculation**:\n",
    "      - The forward pass only computes the loss and accuracy for nodes specified by the `train_mask`, `val_mask`, or `test_mask`.\n",
    "      - **Accuracy Calculation**: The predicted class is compared with the ground truth using `argmax` and accuracy is computed as the percentage of correct predictions.\n",
    "    - **Output**: Returns the computed loss and accuracy for the specified mask.\n",
    "\n",
    "3. **Optimizer Configuration (`configure_optimizers`)**:\n",
    "    - Defines the optimizer for training the model, in this case, Stochastic Gradient Descent (SGD) with momentum and weight decay. Adam can be used as an alternative optimizer.\n",
    "\n",
    "4. **Training Step (`training_step`)**:\n",
    "    - Executes a single training step by calling `forward` in `train` mode and logging the training loss and accuracy using PyTorch Lightning’s `self.log`.\n",
    "\n",
    "5. **Validation Step (`validation_step`)**:\n",
    "    - Executes a validation step by calling `forward` in `val` mode and logs the validation accuracy.\n",
    "\n",
    "6. **Test Step (`test_step`)**:\n",
    "    - Executes a test step by calling `forward` in `test` mode and logs the test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Only calculate the loss on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We use SGD here, but Adam works as well\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ac1f7",
   "metadata": {},
   "source": [
    "### train_node_classifier: Training Node-Level Classifier with PyTorch Lightning\n",
    "\n",
    "This function trains a node-level graph neural network (GNN) classifier using PyTorch Lightning. It supports training from scratch or loading a pretrained model if available.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Seed Setting**:  \n",
    "   The random seed is set using `pl.seed_everything(42)` to ensure reproducibility across runs.\n",
    "\n",
    "2. **DataLoader Initialization**:  \n",
    "   A PyTorch Geometric `DataLoader` is created for the graph dataset, which handles batching. Since this is node-level classification, the batch size is set to `1`.\n",
    "\n",
    "3. **Trainer Setup**:  \n",
    "   A PyTorch Lightning `Trainer` is initialized:\n",
    "   - **Callbacks**: A model checkpoint callback is added to save only the model weights, monitoring the validation accuracy (`val_acc`) and saving the best model.\n",
    "   - **Device Selection**: The accelerator is set to `\"gpu\"` if a CUDA device is available, otherwise it uses the CPU.\n",
    "   - **Epochs**: The maximum number of epochs is set to 200.\n",
    "   - **Progress Bar**: The progress bar is disabled since the epoch size is only 1 (due to the batch size being 1).\n",
    "\n",
    "4. **Pretrained Model Check**:  \n",
    "   The function checks if a pretrained model checkpoint exists in the `CHECKPOINT_PATH`. If found, the model is loaded and training is skipped. If no pretrained model is found, the function initializes a new model and trains it on the dataset.\n",
    "\n",
    "5. **Model Training**:  \n",
    "   If training from scratch:\n",
    "   - A `NodeLevelGNN` model is instantiated with the input and output dimensions corresponding to the dataset's node features and classes.\n",
    "   - The model is trained using `trainer.fit` with both the training and validation data being the same (since it's node-level classification).\n",
    "\n",
    "6. **Testing**:  \n",
    "   Once the best model is identified (via the validation set), it is tested on the dataset using `trainer.test`. The accuracy on the test set is returned.\n",
    "\n",
    "7. **Accuracy Computation**:  \n",
    "   The model's accuracy on the training, validation, and test sets is computed by applying the model in different modes (`train`, `val`, `test`).\n",
    "\n",
    "8. **Return**:  \n",
    "   The function returns the trained model and a dictionary of accuracy results for the training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         enable_progress_bar=False) # False because epoch size is 1\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything()\n",
    "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c35d4",
   "metadata": {},
   "source": [
    "### Training and Evaluating MLP for Node-Level Classification\n",
    "\n",
    "This section calls the `train_node_classifier` function to train an MLP model on the **Cora** dataset, which is commonly used for node-level classification tasks.\n",
    "\n",
    "#### Key Arguments:\n",
    "- **`model_name=\"MLP\"`**: Specifies that a Multi-Layer Perceptron (MLP) model is being used instead of a GNN model.\n",
    "- **`dataset=cora_dataset`**: The Cora dataset is passed as the input graph dataset.\n",
    "- **`c_hidden=16`**: The hidden layer of the MLP has 16 units (or neurons).\n",
    "- **`num_layers=2`**: The MLP will have 2 layers.\n",
    "- **`dp_rate=0.1`**: A dropout rate of 10% is applied to prevent overfitting.\n",
    "\n",
    "#### Function Execution:\n",
    "- **Training**: The function trains the MLP model on the Cora dataset, performing node-level classification. It checks for any pretrained models, trains a new model if needed, and evaluates the model.\n",
    "- **Return Values**: \n",
    "  - `node_mlp_model`: The trained MLP model.\n",
    "  - `node_mlp_result`: A dictionary containing the training, validation, and test accuracies.\n",
    "\n",
    "#### Results Printing:\n",
    "- **`print_results(node_mlp_result)`**: This function prints out the results (accuracy scores) from the training process, showing how well the MLP model performed on the training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelMLP.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train accuracy: 97.14%\n",
      "Val accuracy:   54.60%\n",
      "Test accuracy:  60.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "\n",
    "print_results(node_mlp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30b59f",
   "metadata": {},
   "source": [
    "### Training and Evaluating GNN (GCN) for Node-Level Classification\n",
    "\n",
    "This section calls the `train_node_classifier` function to train a **Graph Convolutional Network (GCN)** on the **Cora** dataset, commonly used for node-level classification tasks.\n",
    "\n",
    "#### Key Arguments:\n",
    "- **`model_name=\"GNN\"`**: Specifies that a Graph Neural Network (GNN) model is being used.\n",
    "- **`layer_name=\"GCN\"`**: Specifies that the GNN architecture to use is the **Graph Convolutional Network (GCN)**.\n",
    "- **`dataset=cora_dataset`**: The Cora dataset is passed as the input graph dataset.\n",
    "- **`c_hidden=16`**: The hidden layers in the GCN have 16 units (or neurons).\n",
    "- **`num_layers=2`**: The GCN will have 2 layers, including the hidden layers and the output layer.\n",
    "- **`dp_rate=0.1`**: A dropout rate of 10% is applied to prevent overfitting.\n",
    "\n",
    "#### Function Execution:\n",
    "- **Training**: The function trains a GCN model on the Cora dataset. It checks if a pretrained model exists and trains a new one if necessary. After training, the model is evaluated on the test set.\n",
    "- **Return Values**:\n",
    "  - `node_gnn_model`: The trained GCN model.\n",
    "  - `node_gnn_result`: A dictionary containing the training, validation, and test accuracies.\n",
    "\n",
    "#### Results Printing:\n",
    "- **`print_results(node_gnn_result)`**: This function prints the performance of the trained GCN model on the training, validation, and test sets, displaying the accuracy scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelGNN.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train accuracy: 100.00%\n",
      "Val accuracy:   78.60%\n",
      "Test accuracy:  82.40%\n"
     ]
    }
   ],
   "source": [
    "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
    "                                                        layer_name=\"GCN\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "print_results(node_gnn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MUTAG Dataset from TUDataset\n",
    "\n",
    "This line initializes the **MUTAG** dataset using the `torch_geometric.datasets.TUDataset` class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbdbf8",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "- **`torch.manual_seed(42)`**: Sets the random seed for reproducibility.\n",
    "- **`tu_dataset.shuffle()`**: Shuffles the MUTAG dataset to randomize the order of the graphs.\n",
    "- **`train_dataset` and `test_dataset`**: Splits the dataset into training (first 150 graphs) and testing sets (remaining graphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tu_dataset.shuffle()\n",
    "train_dataset = tu_dataset[:150]\n",
    "test_dataset = tu_dataset[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae25d3",
   "metadata": {},
   "source": [
    "### DataLoader Setup\n",
    "\n",
    "- **`graph_train_loader`**: Creates a DataLoader for the training dataset with a batch size of 64 and shuffling enabled.\n",
    "- **`graph_val_loader`**: DataLoader for the test dataset (can be used for validation if needed) with a batch size of 64.\n",
    "- **`graph_test_loader`**: DataLoader for the test dataset for final evaluation, also with a batch size of 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphGNNModel: Graph Classification Model\n",
    "\n",
    "- **Initialization (`__init__`)**:  \n",
    "  Combines a GNN model (`GNNModel`) to extract features from the graph, followed by a linear classification head with dropout (`dp_rate_linear`).\n",
    "\n",
    "- **Forward Pass (`forward`)**:  \n",
    "  - **Inputs**:  \n",
    "    - `x`: Node features.\n",
    "    - `edge_index`: Graph edges.\n",
    "    - `batch_idx`: Batch index for each node.\n",
    "  - **Process**:\n",
    "    - Features are passed through the GNN.\n",
    "    - Global mean pooling aggregates node features into graph-level representations.\n",
    "    - The linear head predicts the final output (classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in,\n",
    "                            c_hidden=c_hidden,\n",
    "                            c_out=c_hidden, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d3ba1",
   "metadata": {},
   "source": [
    "### GraphLevelGNN: Graph-Level Classification with PyTorch Lightning\n",
    "\n",
    "- **Initialization (`__init__`)**:  \n",
    "  - A graph-level GNN model (`GraphGNNModel`) is instantiated.\n",
    "  - The loss function is either binary cross-entropy (for binary classification) or cross-entropy (for multi-class classification).\n",
    "\n",
    "- **Forward Pass (`forward`)**:  \n",
    "  - **Inputs**: Graph node features (`x`), edge list (`edge_index`), and batch index (`batch_idx`).\n",
    "  - **Process**:\n",
    "    - Runs node features through the GNN, then applies mean pooling for graph-level representation.\n",
    "    - Classifies the output based on the number of classes (`c_out`).\n",
    "    - Computes loss and accuracy.\n",
    "\n",
    "- **Optimizer (`configure_optimizers`)**:  \n",
    "  - Uses `AdamW` optimizer with a higher learning rate due to the small dataset and model size.\n",
    "\n",
    "- **Training/Validation/Test Steps**:  \n",
    "  - For each step, the model computes the loss and accuracy, and logs the metrics using PyTorch Lightning's `self.log`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396667c",
   "metadata": {},
   "source": [
    "### train_graph_classifier: Graph-Level Classification Training\n",
    "\n",
    "- **Seed Setting**:  \n",
    "  Sets the random seed for reproducibility using `pl.seed_everything(42)`.\n",
    "\n",
    "- **Trainer Setup**:  \n",
    "  Initializes a PyTorch Lightning `Trainer` with:\n",
    "  - A checkpoint callback to save the best model based on validation accuracy (`val_acc`).\n",
    "  - GPU support if available, otherwise runs on CPU.\n",
    "  - A maximum of 500 training epochs.\n",
    "\n",
    "- **Pretrained Model Check**:  \n",
    "  Checks if a pretrained model exists. If found, the model is loaded to skip training. If not, a new `GraphLevelGNN` model is initialized and trained.\n",
    "\n",
    "- **Model Training**:  \n",
    "  The `GraphLevelGNN` is trained on the training set (`graph_train_loader`) and validated on the validation set (`graph_val_loader`).\n",
    "\n",
    "- **Testing**:  \n",
    "  After training, the model is tested on both the training and test sets, and the accuracy for both sets is returned in the result dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=500,\n",
    "                         enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features,\n",
    "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes,\n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39949cf7",
   "metadata": {},
   "source": [
    "### Training and Evaluating GraphConv Model for Graph-Level Classification\n",
    "\n",
    "This section calls the `train_graph_classifier` function to train a **GraphConv** model on a graph-level classification task.\n",
    "\n",
    "#### Function Execution:\n",
    "- The model is trained on the provided dataset and tested on both the training and test sets. The resulting accuracies for both sets are returned in the `result` dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/GraphLevelGraphConv.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/opt/anaconda3/envs/nn_class/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
    "                                       c_hidden=256,\n",
    "                                       layer_name=\"GraphConv\",\n",
    "                                       num_layers=3,\n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 93.28%\n",
      "Test performance:  92.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
